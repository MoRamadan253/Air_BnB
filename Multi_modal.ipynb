{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "228d3fea2ba241a4b12e3d0b137eac47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a00ef3aa1121417783af43a7d932189a",
              "IPY_MODEL_6c55ea3270c64e77ab0594a905992a2e",
              "IPY_MODEL_2566afe077624236b7ae474c6844d809"
            ],
            "layout": "IPY_MODEL_80c48debc1c444d3b07012c00ad1340b"
          }
        },
        "a00ef3aa1121417783af43a7d932189a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_818488c7c2f1480db232cbf16f59eaae",
            "placeholder": "​",
            "style": "IPY_MODEL_6a76dd6ebe1240308e6e46de177fc396",
            "value": "100%"
          }
        },
        "6c55ea3270c64e77ab0594a905992a2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_badc0c967aa942149c1d68c9dadb725d",
            "max": 7360,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a44c72ff6e8c40af9f0dfb230a7640d5",
            "value": 7360
          }
        },
        "2566afe077624236b7ae474c6844d809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_759c42b1dd7f48fd89ffc959fbf8c128",
            "placeholder": "​",
            "style": "IPY_MODEL_7d52df4acdfb46e18d7c5395c36a6cd5",
            "value": " 7360/7360 [01:31&lt;00:00, 79.34it/s]"
          }
        },
        "80c48debc1c444d3b07012c00ad1340b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "818488c7c2f1480db232cbf16f59eaae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a76dd6ebe1240308e6e46de177fc396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "badc0c967aa942149c1d68c9dadb725d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a44c72ff6e8c40af9f0dfb230a7640d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "759c42b1dd7f48fd89ffc959fbf8c128": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d52df4acdfb46e18d7c5395c36a6cd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d47010a71d53401fb62152b3845cac3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5cee952f68634d32a56cbad8c1e5e98f",
              "IPY_MODEL_0ca94012a6354623ab902eeca4b02036",
              "IPY_MODEL_704f3edc74d14a20909c82bcb9e34172"
            ],
            "layout": "IPY_MODEL_023d9c5ff45d4522a883939b29d84608"
          }
        },
        "5cee952f68634d32a56cbad8c1e5e98f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94a94ee09c0343418247de17253355e6",
            "placeholder": "​",
            "style": "IPY_MODEL_5028f22cd9074562a5f92da98d5cea12",
            "value": "100%"
          }
        },
        "0ca94012a6354623ab902eeca4b02036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdff1c1fd8db40c597305f9cd3c62ca1",
            "max": 7360,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_623897cab1714a97b990e8c40786d790",
            "value": 7360
          }
        },
        "704f3edc74d14a20909c82bcb9e34172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75701240738c4182bbbe12cd738550c7",
            "placeholder": "​",
            "style": "IPY_MODEL_a0a6bd128c5849279ba5c173526097c1",
            "value": " 7360/7360 [01:29&lt;00:00, 83.29it/s]"
          }
        },
        "023d9c5ff45d4522a883939b29d84608": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94a94ee09c0343418247de17253355e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5028f22cd9074562a5f92da98d5cea12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdff1c1fd8db40c597305f9cd3c62ca1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "623897cab1714a97b990e8c40786d790": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "75701240738c4182bbbe12cd738550c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0a6bd128c5849279ba5c173526097c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34f67c07504341e5a96bdc6460aad5ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6e5b9e14ce741e394af154859c13c3a",
              "IPY_MODEL_8bb0eb0a3b9b4da7abd19be50fa71b65",
              "IPY_MODEL_2aabb71714e14e22b4dde7728b2b1778"
            ],
            "layout": "IPY_MODEL_c47c147e4dd6430d873d8c7920018523"
          }
        },
        "a6e5b9e14ce741e394af154859c13c3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b14c71a8c4984915adc2752734b9e99b",
            "placeholder": "​",
            "style": "IPY_MODEL_b3cdbed7a354400893e0d6af548146a3",
            "value": "100%"
          }
        },
        "8bb0eb0a3b9b4da7abd19be50fa71b65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e14187d38b524cd198e6495b2be4fdcf",
            "max": 7627,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9bf51b5cd5984e41bdccb3efad4ee9b3",
            "value": 7627
          }
        },
        "2aabb71714e14e22b4dde7728b2b1778": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe07742a0e3548f093a0e8c61c2e8540",
            "placeholder": "​",
            "style": "IPY_MODEL_ec27b96bf0474bb9a145835b3248be72",
            "value": " 7627/7627 [01:24&lt;00:00, 92.72it/s]"
          }
        },
        "c47c147e4dd6430d873d8c7920018523": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b14c71a8c4984915adc2752734b9e99b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3cdbed7a354400893e0d6af548146a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e14187d38b524cd198e6495b2be4fdcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bf51b5cd5984e41bdccb3efad4ee9b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe07742a0e3548f093a0e8c61c2e8540": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec27b96bf0474bb9a145835b3248be72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoRamadan253/Air_BnB/blob/main/Multi_modal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Problem Formulation:***"
      ],
      "metadata": {
        "id": "WDk-QLZC21up"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this challenge, our aim is to predict the listing price for different apartments/houses on airbnb for different areas in Montreal.\n",
        "\n",
        "1-Input: Here we have a dataset of 7627 rows and 4 columns including summary of the property, an image of it, its type (apartment/house/loft...etc) and its price.\n",
        "\n",
        "2-Output: Using the available information about each property, we want to predict its price.\n",
        "\n",
        "3-Data Mining Challenges: Here the data is more challenging as compared to previous challenges because it contains images and text data, not numeric data types. So the challenge here would be to be able to work with those data types and make a model to predict the prices."
      ],
      "metadata": {
        "id": "ImgBDb8Q26Lc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Code Documentation:***"
      ],
      "metadata": {
        "id": "Us7RzKBN7nea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we use Kaggle API to download the data"
      ],
      "metadata": {
        "id": "vh6GN5HW7wgM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsbewfC7ecfb",
        "outputId": "8dcd963e-556b-4e4a-f03b-376b81de5702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading cisc-873-dm-f22-a4.zip to /content\n",
            " 99% 595M/604M [00:04<00:00, 144MB/s]\n",
            "100% 604M/604M [00:04<00:00, 150MB/s]\n"
          ]
        }
      ],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c cisc-873-dm-f22-a4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip -q '/content/cisc-873-dm-f22-a4.zip'"
      ],
      "metadata": {
        "id": "pa7UGuUEwwQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we import the needed libraries"
      ],
      "metadata": {
        "id": "khc5wW3U8EXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Conv2D,Flatten,Dense,MaxPool2D,Dropout,Conv1D,GlobalMaxPooling1D,GRU,LSTM,MaxPooling1D,Bidirectional,SimpleRNN\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pprint import pprint\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam,RMSprop\n"
      ],
      "metadata": {
        "id": "OobKuz7gfvt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We read the training and test datasets"
      ],
      "metadata": {
        "id": "GNGHKFw28PEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/a4/train_xy.csv')"
      ],
      "metadata": {
        "id": "YRKv7AQjfpn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('/content/a4/test_x.csv')"
      ],
      "metadata": {
        "id": "LtChXmFV9arY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "km0YXr5eg1G2",
        "outputId": "2e5d365e-0218-4534-abb1-de46093fe192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             summary            image  \\\n",
              "0  Spacious, sunny and cozy modern apartment in t...  img_train/0.jpg   \n",
              "1  Located in one of the most vibrant and accessi...  img_train/1.jpg   \n",
              "2  Logement coquet et douillet à 10 minutes du ce...  img_train/2.jpg   \n",
              "3  Beautiful and spacious (1076 sc ft, / 100 mc) ...  img_train/3.jpg   \n",
              "4  Très grand appartement ''rustique'' et très ag...  img_train/4.jpg   \n",
              "\n",
              "        type  price  \n",
              "0  Apartment      1  \n",
              "1  Apartment      0  \n",
              "2  Apartment      1  \n",
              "3  Apartment      1  \n",
              "4  Apartment      0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7fc9d69e-9228-4b0e-b5ef-6ebbc7008718\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summary</th>\n",
              "      <th>image</th>\n",
              "      <th>type</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Spacious, sunny and cozy modern apartment in t...</td>\n",
              "      <td>img_train/0.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Located in one of the most vibrant and accessi...</td>\n",
              "      <td>img_train/1.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Logement coquet et douillet à 10 minutes du ce...</td>\n",
              "      <td>img_train/2.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Beautiful and spacious (1076 sc ft, / 100 mc) ...</td>\n",
              "      <td>img_train/3.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Très grand appartement ''rustique'' et très ag...</td>\n",
              "      <td>img_train/4.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7fc9d69e-9228-4b0e-b5ef-6ebbc7008718')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7fc9d69e-9228-4b0e-b5ef-6ebbc7008718 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7fc9d69e-9228-4b0e-b5ef-6ebbc7008718');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxvt13YyjGTX",
        "outputId": "52c1450b-33ef-478b-a94c-fb7ca8732928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7627, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We check for null values in our training dataset"
      ],
      "metadata": {
        "id": "02l1BjMe83Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRrjVBPOqg1A",
        "outputId": "4d549755-1490-4257-f20e-d0cf3e841fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "summary    301\n",
              "image        0\n",
              "type         0\n",
              "price        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we choose not to drop rows with empty summary since the corresponding image column is not empty so we could make use of those entries in our multimodal trials"
      ],
      "metadata": {
        "id": "pjzk6nO-9RmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the load_image function to read the images file and transform them into an array of dimensions (64,64,2)"
      ],
      "metadata": {
        "id": "8f-NRiWm-Ab9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(file):\n",
        "    try:\n",
        "        image = Image.open(\n",
        "            '/content/a4/' + file\n",
        "        ).convert('LA').resize((64, 64))  #LA means black and white images with alpha\n",
        "        arr = np.array(image)\n",
        "    except:\n",
        "        arr = np.zeros((64, 64, 2)) \n",
        "    return arr"
      ],
      "metadata": {
        "id": "CFNjU2tNusKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we drop duplicated rows"
      ],
      "metadata": {
        "id": "qJrMMMLv-gRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop_duplicates()"
      ],
      "metadata": {
        "id": "W3m7B2h3KIu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRnD9G6uMp7y",
        "outputId": "2b19b002-9815-4f25-fd3b-ed74768e788c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7627, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['price'].value_counts()  #Checking the number of rows for each unique price "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLDYytG2Mq8P",
        "outputId": "1ec137d3-4a13-4174-fd51-d4bc35a0b898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4737\n",
              "1    2403\n",
              "2     487\n",
              "Name: price, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['type'].value_counts()  #Checking the number of rows for each unique property type"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ri7Zp_4lNFv2",
        "outputId": "6163e635-63ed-4343-fc3b-572251fcc91e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Apartment                 5765\n",
              "Condominium                691\n",
              "House                      406\n",
              "Loft                       324\n",
              "Townhouse                  167\n",
              "Serviced apartment          77\n",
              "Bed and breakfast           38\n",
              "Guest suite                 32\n",
              "Hostel                      26\n",
              "Bungalow                    25\n",
              "Guesthouse                  14\n",
              "Cottage                     12\n",
              "Aparthotel                  12\n",
              "Boutique hotel              10\n",
              "Other                        8\n",
              "Villa                        7\n",
              "Tiny house                   3\n",
              "Boat                         2\n",
              "Cabin                        2\n",
              "Camper/RV                    2\n",
              "Casa particular (Cuba)       1\n",
              "Hotel                        1\n",
              "Earth house                  1\n",
              "Castle                       1\n",
              "Name: type, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['type'] = df['type'].astype('category').cat.codes  #converting type column to numeric values\n",
        "len_price = len(df.price.unique())                    #Getting the number of unique values for both type and price to be used later\n",
        "len_type = len(df['type'].unique())"
      ],
      "metadata": {
        "id": "TrzeKMXCYVS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JH9B17TFa81D",
        "outputId": "3ba746ef-b71a-4eac-ffc8-6d68f733c3e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 7627 entries, 0 to 7626\n",
            "Data columns (total 4 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   summary  7326 non-null   object\n",
            " 1   image    7627 non-null   object\n",
            " 2   type     7627 non-null   int8  \n",
            " 3   price    7627 non-null   int64 \n",
            "dtypes: int64(1), int8(1), object(2)\n",
            "memory usage: 245.8+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we are going to build a multimodal and multitasking model so we use the image and summary columns as inputs and type and price columns as outputs"
      ],
      "metadata": {
        "id": "ytBEXEFNFGrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_image = np.array([load_image(i) for i in tqdm(df['image'])])\n",
        "\n",
        "x_train_text = df.summary.astype('str')\n",
        "\n",
        "y_train_type = df['type']\n",
        "\n",
        "y_train_price = df.price"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "34f67c07504341e5a96bdc6460aad5ba",
            "a6e5b9e14ce741e394af154859c13c3a",
            "8bb0eb0a3b9b4da7abd19be50fa71b65",
            "2aabb71714e14e22b4dde7728b2b1778",
            "c47c147e4dd6430d873d8c7920018523",
            "b14c71a8c4984915adc2752734b9e99b",
            "b3cdbed7a354400893e0d6af548146a3",
            "e14187d38b524cd198e6495b2be4fdcf",
            "9bf51b5cd5984e41bdccb3efad4ee9b3",
            "fe07742a0e3548f093a0e8c61c2e8540",
            "ec27b96bf0474bb9a145835b3248be72"
          ]
        },
        "id": "3P76MtVNhbNT",
        "outputId": "409fff86-dfc6-4185-99eb-ee035a68c310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7627 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34f67c07504341e5a96bdc6460aad5ba"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique = set(x_train_text.str.replace('[^a-zA-Z ]', '').str.lower().str.split(' ').sum()) \n",
        "\n",
        "print(len(list(sorted(unique))))      #We calculate the total number of unique words in our dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No7wSMeui7qb",
        "outputId": "43237393-1855-4916-aff6-8d6a7c69aadb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 40000\n",
        "max_len = 100\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "\n",
        "# build vocabulary from training set\n",
        "\n",
        "def _preprocess(list_of_text):\n",
        "  \n",
        "  tokenizer.fit_on_texts(list_of_text)\n",
        "  return pad_sequences(\n",
        "      tokenizer.texts_to_sequences(list_of_text), \n",
        "      maxlen=max_len, \n",
        "      padding='post',\n",
        "  )\n",
        "    "
      ],
      "metadata": {
        "id": "BIeWtM5suq7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Trial 1: ***\n",
        "\n",
        "For the first trial, we are going to use the model used in the previous lab to create a baseline model then start tuning the hyperparameters for better results. \n",
        "\n",
        "Here we use both the image and summary columns as imputs. For the image column we use a convolutional layer and a pooling layer and for the summary column we use the generated embeddings of our tokens represented in a vector space of 100 dimensions.\n",
        "\n",
        "Adam is used as an optimizer and sparse categorical loss is our loss metric"
      ],
      "metadata": {
        "id": "HsbAZ0-hF2QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_text = keras.Input(batch_shape=(None, max_len)) \n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "\n",
        "\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text) #vector space of 100 dimensions for each word\n",
        "averaged = tf.reduce_mean(embedded, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "cov = Conv2D(32, (16, 16))(in_image)  #16*16 kernel \n",
        "pl = MaxPool2D((16, 16))(cov)\n",
        "flattened = Flatten()(pl)\n",
        "\n",
        "\n",
        "fused = tf.concat([averaged, flattened], axis=-1)\n",
        "\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "\n",
        "\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.5,\n",
        "        'price': 0.5,\n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4Tp1LrzkeJI",
        "outputId": "7380ef9b-eddb-4b81-ab93-1b892d9d51c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_6 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 49, 49, 32)   16416       ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 128, 100)     800000      ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 3, 3, 32)    0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_2 (TFOpLam  (None, 100)         0           ['embedding_2[0][0]']            \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 288)          0           ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " tf.concat_2 (TFOpLambda)       (None, 388)          0           ['tf.math.reduce_mean_2[0][0]',  \n",
            "                                                                  'flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            1167        ['tf.concat_2[0][0]']            \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           9336        ['tf.concat_2[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 826,919\n",
            "Trainable params: 826,919\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_genre_loss', patience=5, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s8Hbg6tqF7d",
        "outputId": "87b2b3e8-b59a-4d3d-ee18-a923a97cefb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 22.5488 - price_loss: 17.8652 - type_loss: 27.2324 - price_sparse_categorical_accuracy: 0.5117 - type_sparse_categorical_accuracy: 0.5791WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 60s 155ms/step - loss: 22.5488 - price_loss: 17.8652 - type_loss: 27.2324 - price_sparse_categorical_accuracy: 0.5117 - type_sparse_categorical_accuracy: 0.5791 - val_loss: 8.4190 - val_price_loss: 8.8369 - val_type_loss: 8.0010 - val_price_sparse_categorical_accuracy: 0.2962 - val_type_sparse_categorical_accuracy: 0.7241\n",
            "Epoch 2/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 10.5035 - price_loss: 7.4705 - type_loss: 13.5364 - price_sparse_categorical_accuracy: 0.5061 - type_sparse_categorical_accuracy: 0.5879WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 58s 153ms/step - loss: 10.5035 - price_loss: 7.4705 - type_loss: 13.5364 - price_sparse_categorical_accuracy: 0.5061 - type_sparse_categorical_accuracy: 0.5879 - val_loss: 5.5290 - val_price_loss: 4.2303 - val_type_loss: 6.8277 - val_price_sparse_categorical_accuracy: 0.5426 - val_type_sparse_categorical_accuracy: 0.6232\n",
            "Epoch 3/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 5.4088 - price_loss: 3.9234 - type_loss: 6.8942 - price_sparse_categorical_accuracy: 0.5232 - type_sparse_categorical_accuracy: 0.5951WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 56s 147ms/step - loss: 5.4088 - price_loss: 3.9234 - type_loss: 6.8942 - price_sparse_categorical_accuracy: 0.5232 - type_sparse_categorical_accuracy: 0.5951 - val_loss: 10.2184 - val_price_loss: 8.7703 - val_type_loss: 11.6666 - val_price_sparse_categorical_accuracy: 0.3493 - val_type_sparse_categorical_accuracy: 0.7012\n",
            "Epoch 4/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 6.7900 - price_loss: 5.1254 - type_loss: 8.4546 - price_sparse_categorical_accuracy: 0.5332 - type_sparse_categorical_accuracy: 0.5947WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 55s 143ms/step - loss: 6.7900 - price_loss: 5.1254 - type_loss: 8.4546 - price_sparse_categorical_accuracy: 0.5332 - type_sparse_categorical_accuracy: 0.5947 - val_loss: 9.4781 - val_price_loss: 8.0246 - val_type_loss: 10.9317 - val_price_sparse_categorical_accuracy: 0.6081 - val_type_sparse_categorical_accuracy: 0.4679\n",
            "Epoch 5/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 5.9551 - price_loss: 4.7452 - type_loss: 7.1650 - price_sparse_categorical_accuracy: 0.5456 - type_sparse_categorical_accuracy: 0.6053WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 55s 143ms/step - loss: 5.9551 - price_loss: 4.7452 - type_loss: 7.1650 - price_sparse_categorical_accuracy: 0.5456 - type_sparse_categorical_accuracy: 0.6053 - val_loss: 3.9640 - val_price_loss: 3.1588 - val_type_loss: 4.7691 - val_price_sparse_categorical_accuracy: 0.5472 - val_type_sparse_categorical_accuracy: 0.5793\n",
            "Epoch 6/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 4.3883 - price_loss: 3.2319 - type_loss: 5.5447 - price_sparse_categorical_accuracy: 0.5763 - type_sparse_categorical_accuracy: 0.6083WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 55s 143ms/step - loss: 4.3883 - price_loss: 3.2319 - type_loss: 5.5447 - price_sparse_categorical_accuracy: 0.5763 - type_sparse_categorical_accuracy: 0.6083 - val_loss: 4.4992 - val_price_loss: 3.6714 - val_type_loss: 5.3269 - val_price_sparse_categorical_accuracy: 0.4934 - val_type_sparse_categorical_accuracy: 0.6153\n",
            "Epoch 7/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 14.3301 - price_loss: 10.2822 - type_loss: 18.3779 - price_sparse_categorical_accuracy: 0.5345 - type_sparse_categorical_accuracy: 0.6040WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 55s 143ms/step - loss: 14.3301 - price_loss: 10.2822 - type_loss: 18.3779 - price_sparse_categorical_accuracy: 0.5345 - type_sparse_categorical_accuracy: 0.6040 - val_loss: 7.1349 - val_price_loss: 4.2419 - val_type_loss: 10.0278 - val_price_sparse_categorical_accuracy: 0.5675 - val_type_sparse_categorical_accuracy: 0.5832\n",
            "Epoch 8/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 7.9240 - price_loss: 5.0024 - type_loss: 10.8456 - price_sparse_categorical_accuracy: 0.5756 - type_sparse_categorical_accuracy: 0.6097WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 56s 146ms/step - loss: 7.9240 - price_loss: 5.0024 - type_loss: 10.8456 - price_sparse_categorical_accuracy: 0.5756 - type_sparse_categorical_accuracy: 0.6097 - val_loss: 8.2374 - val_price_loss: 5.1148 - val_type_loss: 11.3600 - val_price_sparse_categorical_accuracy: 0.5197 - val_type_sparse_categorical_accuracy: 0.7471\n",
            "Epoch 9/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 7.7312 - price_loss: 4.4683 - type_loss: 10.9940 - price_sparse_categorical_accuracy: 0.5883 - type_sparse_categorical_accuracy: 0.6173WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 55s 145ms/step - loss: 7.7312 - price_loss: 4.4683 - type_loss: 10.9940 - price_sparse_categorical_accuracy: 0.5883 - type_sparse_categorical_accuracy: 0.6173 - val_loss: 7.3863 - val_price_loss: 3.8671 - val_type_loss: 10.9055 - val_price_sparse_categorical_accuracy: 0.5013 - val_type_sparse_categorical_accuracy: 0.1913\n",
            "Epoch 10/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 8.3810 - price_loss: 5.0419 - type_loss: 11.7201 - price_sparse_categorical_accuracy: 0.6024 - type_sparse_categorical_accuracy: 0.6225WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 55s 144ms/step - loss: 8.3810 - price_loss: 5.0419 - type_loss: 11.7201 - price_sparse_categorical_accuracy: 0.6024 - type_sparse_categorical_accuracy: 0.6225 - val_loss: 10.9678 - val_price_loss: 5.2919 - val_type_loss: 16.6437 - val_price_sparse_categorical_accuracy: 0.6042 - val_type_sparse_categorical_accuracy: 0.7615\n",
            "Epoch 11/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 19.0483 - price_loss: 11.4555 - type_loss: 26.6410 - price_sparse_categorical_accuracy: 0.5788 - type_sparse_categorical_accuracy: 0.6051WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 57s 150ms/step - loss: 19.0483 - price_loss: 11.4555 - type_loss: 26.6410 - price_sparse_categorical_accuracy: 0.5788 - type_sparse_categorical_accuracy: 0.6051 - val_loss: 61.7242 - val_price_loss: 64.2955 - val_type_loss: 59.1529 - val_price_sparse_categorical_accuracy: 0.6245 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 12/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 12.4775 - price_loss: 7.1204 - type_loss: 17.8346 - price_sparse_categorical_accuracy: 0.6056 - type_sparse_categorical_accuracy: 0.6314WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 58s 151ms/step - loss: 12.4775 - price_loss: 7.1204 - type_loss: 17.8346 - price_sparse_categorical_accuracy: 0.6056 - type_sparse_categorical_accuracy: 0.6314 - val_loss: 4.9435 - val_price_loss: 3.2433 - val_type_loss: 6.6437 - val_price_sparse_categorical_accuracy: 0.6225 - val_type_sparse_categorical_accuracy: 0.6959\n",
            "Epoch 13/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 5.7791 - price_loss: 3.1129 - type_loss: 8.4454 - price_sparse_categorical_accuracy: 0.6601 - type_sparse_categorical_accuracy: 0.6573WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 57s 150ms/step - loss: 5.7791 - price_loss: 3.1129 - type_loss: 8.4454 - price_sparse_categorical_accuracy: 0.6601 - type_sparse_categorical_accuracy: 0.6573 - val_loss: 49.1543 - val_price_loss: 30.7039 - val_type_loss: 67.6047 - val_price_sparse_categorical_accuracy: 0.1855 - val_type_sparse_categorical_accuracy: 0.2857\n",
            "Epoch 14/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 7.5873 - price_loss: 4.4948 - type_loss: 10.6799 - price_sparse_categorical_accuracy: 0.6347 - type_sparse_categorical_accuracy: 0.6499WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 57s 149ms/step - loss: 7.5873 - price_loss: 4.4948 - type_loss: 10.6799 - price_sparse_categorical_accuracy: 0.6347 - type_sparse_categorical_accuracy: 0.6499 - val_loss: 5.9111 - val_price_loss: 3.3467 - val_type_loss: 8.4755 - val_price_sparse_categorical_accuracy: 0.5505 - val_type_sparse_categorical_accuracy: 0.7497\n",
            "Epoch 15/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 4.8742 - price_loss: 2.9730 - type_loss: 6.7754 - price_sparse_categorical_accuracy: 0.6569 - type_sparse_categorical_accuracy: 0.6684WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 57s 148ms/step - loss: 4.8742 - price_loss: 2.9730 - type_loss: 6.7754 - price_sparse_categorical_accuracy: 0.6569 - type_sparse_categorical_accuracy: 0.6684 - val_loss: 4.4794 - val_price_loss: 3.3762 - val_type_loss: 5.5826 - val_price_sparse_categorical_accuracy: 0.5799 - val_type_sparse_categorical_accuracy: 0.6389\n",
            "Epoch 16/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 3.5470 - price_loss: 2.4095 - type_loss: 4.6845 - price_sparse_categorical_accuracy: 0.6763 - type_sparse_categorical_accuracy: 0.6828WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 58s 151ms/step - loss: 3.5470 - price_loss: 2.4095 - type_loss: 4.6845 - price_sparse_categorical_accuracy: 0.6763 - type_sparse_categorical_accuracy: 0.6828 - val_loss: 3.8153 - val_price_loss: 3.1004 - val_type_loss: 4.5301 - val_price_sparse_categorical_accuracy: 0.5478 - val_type_sparse_categorical_accuracy: 0.7248\n",
            "Epoch 17/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 5.2875 - price_loss: 3.8043 - type_loss: 6.7708 - price_sparse_categorical_accuracy: 0.6648 - type_sparse_categorical_accuracy: 0.6825WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 57s 149ms/step - loss: 5.2875 - price_loss: 3.8043 - type_loss: 6.7708 - price_sparse_categorical_accuracy: 0.6648 - type_sparse_categorical_accuracy: 0.6825 - val_loss: 13.2496 - val_price_loss: 4.7756 - val_type_loss: 21.7237 - val_price_sparse_categorical_accuracy: 0.5957 - val_type_sparse_categorical_accuracy: 0.1540\n",
            "Epoch 18/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 18.5785 - price_loss: 13.1258 - type_loss: 24.0312 - price_sparse_categorical_accuracy: 0.6068 - type_sparse_categorical_accuracy: 0.6296WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 55s 144ms/step - loss: 18.5785 - price_loss: 13.1258 - type_loss: 24.0312 - price_sparse_categorical_accuracy: 0.6068 - type_sparse_categorical_accuracy: 0.6296 - val_loss: 15.8243 - val_price_loss: 13.1879 - val_type_loss: 18.4606 - val_price_sparse_categorical_accuracy: 0.2123 - val_type_sparse_categorical_accuracy: 0.7490\n",
            "Epoch 19/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 9.8543 - price_loss: 5.8466 - type_loss: 13.8621 - price_sparse_categorical_accuracy: 0.6587 - type_sparse_categorical_accuracy: 0.6738WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 56s 147ms/step - loss: 9.8543 - price_loss: 5.8466 - type_loss: 13.8621 - price_sparse_categorical_accuracy: 0.6587 - type_sparse_categorical_accuracy: 0.6738 - val_loss: 4.6086 - val_price_loss: 3.1289 - val_type_loss: 6.0883 - val_price_sparse_categorical_accuracy: 0.6094 - val_type_sparse_categorical_accuracy: 0.7058\n",
            "Epoch 20/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 3.7066 - price_loss: 1.9408 - type_loss: 5.4724 - price_sparse_categorical_accuracy: 0.7127 - type_sparse_categorical_accuracy: 0.7027WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 55s 143ms/step - loss: 3.7066 - price_loss: 1.9408 - type_loss: 5.4724 - price_sparse_categorical_accuracy: 0.7127 - type_sparse_categorical_accuracy: 0.7027 - val_loss: 5.3826 - val_price_loss: 3.5821 - val_type_loss: 7.1831 - val_price_sparse_categorical_accuracy: 0.5649 - val_type_sparse_categorical_accuracy: 0.6796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_image = np.array([load_image(i) for i in tqdm(df_test.image)])\n",
        "\n",
        "x_test_text = _preprocess(df_test.summary.astype('str'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "228d3fea2ba241a4b12e3d0b137eac47",
            "a00ef3aa1121417783af43a7d932189a",
            "6c55ea3270c64e77ab0594a905992a2e",
            "2566afe077624236b7ae474c6844d809",
            "80c48debc1c444d3b07012c00ad1340b",
            "818488c7c2f1480db232cbf16f59eaae",
            "6a76dd6ebe1240308e6e46de177fc396",
            "badc0c967aa942149c1d68c9dadb725d",
            "a44c72ff6e8c40af9f0dfb230a7640d5",
            "759c42b1dd7f48fd89ffc959fbf8c128",
            "7d52df4acdfb46e18d7c5395c36a6cd5"
          ]
        },
        "id": "ER2HKK7pqalA",
        "outputId": "f2aeb804-5e9a-49b3-a927-b18056a41635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7360 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "228d3fea2ba241a4b12e3d0b137eac47"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict = model.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "price_predicted = y_predict['price'] \n",
        "type_predicted = y_predict['type'] \n",
        "\n",
        "# categories\n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "type_category_predicted = np.argmax(type_predicted, axis=1)"
      ],
      "metadata": {
        "id": "afV7UDRluZDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(\n",
        "    {'id': df_test.id,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "--yZMZnd1O0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Impact:***\n",
        "\n",
        "The first submission got an accuracy of 50% on Kaggle, which is not the best result so now we are going to tune the hyperparemeters we have"
      ],
      "metadata": {
        "id": "FaKxBFz0JM79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Trial 2:***\n",
        "\n",
        "For the second trial, we thought of using a dictionary size of only 14K words (instead of 40K) since the total number of unique words in our dataset was nearly 13K. Moreover, we used a vector space of 200 dimensions (as compared to 100 dimensions in the previous trial)."
      ],
      "metadata": {
        "id": "0l5BwJEtI_uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Tokenizer\n",
        "vocab_size = 14000\n",
        "max_len = 128\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "x_train_text_id = _preprocess(x_train_text)\n",
        "\n",
        "##Model Building\n",
        "in_text = keras.Input(batch_shape=(None, max_len)) \n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 200)(in_text) \n",
        "averaged = tf.reduce_mean(embedded, axis=1)\n",
        "\n",
        "cov = Conv2D(32, (16, 16))(in_image)\n",
        "pl = MaxPool2D((16, 16))(cov)\n",
        "flattened = Flatten()(pl)\n",
        "fused = tf.concat([averaged, flattened], axis=-1)\n",
        "\n",
        "\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.5,\n",
        "        'price': 0.5,\n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "##Model Training\n",
        "history = model.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_genre_loss', patience=5, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "##Model Testing and making predictions\n",
        "x_test_text = _preprocess(df_test.summary.astype('str'))\n",
        "y_predict = model.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "price_predicted = y_predict['price'] \n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "\n",
        "pd.DataFrame(\n",
        "    {'id': df_test.id,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0gxpdzXHrD1",
        "outputId": "f31e5fcb-97a7-4dfe-c407-af3082f68af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 24.6257 - price_loss: 17.3977 - type_loss: 31.8537 - price_sparse_categorical_accuracy: 0.4943 - type_sparse_categorical_accuracy: 0.5870WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 64s 163ms/step - loss: 24.6257 - price_loss: 17.3977 - type_loss: 31.8537 - price_sparse_categorical_accuracy: 0.4943 - type_sparse_categorical_accuracy: 0.5870 - val_loss: 10.0403 - val_price_loss: 9.4759 - val_type_loss: 10.6048 - val_price_sparse_categorical_accuracy: 0.2615 - val_type_sparse_categorical_accuracy: 0.7549\n",
            "Epoch 2/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 6.5041 - price_loss: 4.6187 - type_loss: 8.3894 - price_sparse_categorical_accuracy: 0.5176 - type_sparse_categorical_accuracy: 0.5927WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 59s 155ms/step - loss: 6.5041 - price_loss: 4.6187 - type_loss: 8.3894 - price_sparse_categorical_accuracy: 0.5176 - type_sparse_categorical_accuracy: 0.5927 - val_loss: 4.6044 - val_price_loss: 3.0883 - val_type_loss: 6.1204 - val_price_sparse_categorical_accuracy: 0.5328 - val_type_sparse_categorical_accuracy: 0.7274\n",
            "Epoch 3/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 7.1434 - price_loss: 5.8071 - type_loss: 8.4796 - price_sparse_categorical_accuracy: 0.5211 - type_sparse_categorical_accuracy: 0.5943WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 59s 155ms/step - loss: 7.1434 - price_loss: 5.8071 - type_loss: 8.4796 - price_sparse_categorical_accuracy: 0.5211 - type_sparse_categorical_accuracy: 0.5943 - val_loss: 18.3523 - val_price_loss: 17.2665 - val_type_loss: 19.4381 - val_price_sparse_categorical_accuracy: 0.3552 - val_type_sparse_categorical_accuracy: 0.5950\n",
            "Epoch 4/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 7.5269 - price_loss: 4.6400 - type_loss: 10.4139 - price_sparse_categorical_accuracy: 0.5453 - type_sparse_categorical_accuracy: 0.6025WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 62s 162ms/step - loss: 7.5269 - price_loss: 4.6400 - type_loss: 10.4139 - price_sparse_categorical_accuracy: 0.5453 - type_sparse_categorical_accuracy: 0.6025 - val_loss: 5.1956 - val_price_loss: 3.4428 - val_type_loss: 6.9484 - val_price_sparse_categorical_accuracy: 0.5138 - val_type_sparse_categorical_accuracy: 0.4351\n",
            "Epoch 5/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 9.1378 - price_loss: 6.4233 - type_loss: 11.8522 - price_sparse_categorical_accuracy: 0.5497 - type_sparse_categorical_accuracy: 0.6073WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 64s 169ms/step - loss: 9.1378 - price_loss: 6.4233 - type_loss: 11.8522 - price_sparse_categorical_accuracy: 0.5497 - type_sparse_categorical_accuracy: 0.6073 - val_loss: 11.9463 - val_price_loss: 5.7781 - val_type_loss: 18.1145 - val_price_sparse_categorical_accuracy: 0.6075 - val_type_sparse_categorical_accuracy: 0.3506\n",
            "Epoch 6/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 11.1292 - price_loss: 7.7930 - type_loss: 14.4654 - price_sparse_categorical_accuracy: 0.5696 - type_sparse_categorical_accuracy: 0.6068WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 64s 167ms/step - loss: 11.1292 - price_loss: 7.7930 - type_loss: 14.4654 - price_sparse_categorical_accuracy: 0.5696 - type_sparse_categorical_accuracy: 0.6068 - val_loss: 9.1310 - val_price_loss: 6.3624 - val_type_loss: 11.8997 - val_price_sparse_categorical_accuracy: 0.4751 - val_type_sparse_categorical_accuracy: 0.5806\n",
            "Epoch 7/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 10.7791 - price_loss: 6.3348 - type_loss: 15.2233 - price_sparse_categorical_accuracy: 0.5814 - type_sparse_categorical_accuracy: 0.6217WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 64s 168ms/step - loss: 10.7791 - price_loss: 6.3348 - type_loss: 15.2233 - price_sparse_categorical_accuracy: 0.5814 - type_sparse_categorical_accuracy: 0.6217 - val_loss: 3.7536 - val_price_loss: 2.9878 - val_type_loss: 4.5194 - val_price_sparse_categorical_accuracy: 0.6035 - val_type_sparse_categorical_accuracy: 0.6166\n",
            "Epoch 8/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 4.0660 - price_loss: 2.5490 - type_loss: 5.5830 - price_sparse_categorical_accuracy: 0.6424 - type_sparse_categorical_accuracy: 0.6548WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 69s 181ms/step - loss: 4.0660 - price_loss: 2.5490 - type_loss: 5.5830 - price_sparse_categorical_accuracy: 0.6424 - type_sparse_categorical_accuracy: 0.6548 - val_loss: 6.9318 - val_price_loss: 3.7004 - val_type_loss: 10.1632 - val_price_sparse_categorical_accuracy: 0.5799 - val_type_sparse_categorical_accuracy: 0.6743\n",
            "Epoch 9/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 3.8627 - price_loss: 2.6037 - type_loss: 5.1217 - price_sparse_categorical_accuracy: 0.6456 - type_sparse_categorical_accuracy: 0.6596WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 66s 173ms/step - loss: 3.8627 - price_loss: 2.6037 - type_loss: 5.1217 - price_sparse_categorical_accuracy: 0.6456 - type_sparse_categorical_accuracy: 0.6596 - val_loss: 29.7307 - val_price_loss: 7.9032 - val_type_loss: 51.5582 - val_price_sparse_categorical_accuracy: 0.5872 - val_type_sparse_categorical_accuracy: 0.0531\n",
            "Epoch 10/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 7.1724 - price_loss: 4.1270 - type_loss: 10.2177 - price_sparse_categorical_accuracy: 0.6330 - type_sparse_categorical_accuracy: 0.6376WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 64s 168ms/step - loss: 7.1724 - price_loss: 4.1270 - type_loss: 10.2177 - price_sparse_categorical_accuracy: 0.6330 - type_sparse_categorical_accuracy: 0.6376 - val_loss: 11.5847 - val_price_loss: 7.6005 - val_type_loss: 15.5689 - val_price_sparse_categorical_accuracy: 0.4115 - val_type_sparse_categorical_accuracy: 0.6422\n",
            "Epoch 11/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 10.5861 - price_loss: 5.8434 - type_loss: 15.3289 - price_sparse_categorical_accuracy: 0.6084 - type_sparse_categorical_accuracy: 0.6325WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 60s 156ms/step - loss: 10.5861 - price_loss: 5.8434 - type_loss: 15.3289 - price_sparse_categorical_accuracy: 0.6084 - type_sparse_categorical_accuracy: 0.6325 - val_loss: 6.2942 - val_price_loss: 4.8631 - val_type_loss: 7.7253 - val_price_sparse_categorical_accuracy: 0.6062 - val_type_sparse_categorical_accuracy: 0.7221\n",
            "Epoch 12/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 6.9556 - price_loss: 4.3147 - type_loss: 9.5965 - price_sparse_categorical_accuracy: 0.6465 - type_sparse_categorical_accuracy: 0.6609WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 60s 156ms/step - loss: 6.9556 - price_loss: 4.3147 - type_loss: 9.5965 - price_sparse_categorical_accuracy: 0.6465 - type_sparse_categorical_accuracy: 0.6609 - val_loss: 9.1620 - val_price_loss: 3.2371 - val_type_loss: 15.0869 - val_price_sparse_categorical_accuracy: 0.5970 - val_type_sparse_categorical_accuracy: 0.1684\n",
            "Epoch 13/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 5.7761 - price_loss: 3.8532 - type_loss: 7.6990 - price_sparse_categorical_accuracy: 0.6494 - type_sparse_categorical_accuracy: 0.6763WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 59s 156ms/step - loss: 5.7761 - price_loss: 3.8532 - type_loss: 7.6990 - price_sparse_categorical_accuracy: 0.6494 - type_sparse_categorical_accuracy: 0.6763 - val_loss: 5.6020 - val_price_loss: 3.9993 - val_type_loss: 7.2046 - val_price_sparse_categorical_accuracy: 0.6311 - val_type_sparse_categorical_accuracy: 0.5282\n",
            "Epoch 14/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 6.1063 - price_loss: 3.2669 - type_loss: 8.9456 - price_sparse_categorical_accuracy: 0.6853 - type_sparse_categorical_accuracy: 0.6738WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 59s 156ms/step - loss: 6.1063 - price_loss: 3.2669 - type_loss: 8.9456 - price_sparse_categorical_accuracy: 0.6853 - type_sparse_categorical_accuracy: 0.6738 - val_loss: 4.3059 - val_price_loss: 3.1485 - val_type_loss: 5.4634 - val_price_sparse_categorical_accuracy: 0.5524 - val_type_sparse_categorical_accuracy: 0.5865\n",
            "Epoch 15/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 3.0273 - price_loss: 1.9449 - type_loss: 4.1096 - price_sparse_categorical_accuracy: 0.7119 - type_sparse_categorical_accuracy: 0.7055WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 60s 156ms/step - loss: 3.0273 - price_loss: 1.9449 - type_loss: 4.1096 - price_sparse_categorical_accuracy: 0.7119 - type_sparse_categorical_accuracy: 0.7055 - val_loss: 4.3673 - val_price_loss: 3.6287 - val_type_loss: 5.1059 - val_price_sparse_categorical_accuracy: 0.5374 - val_type_sparse_categorical_accuracy: 0.7097\n",
            "Epoch 16/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 3.5632 - price_loss: 2.4553 - type_loss: 4.6712 - price_sparse_categorical_accuracy: 0.7051 - type_sparse_categorical_accuracy: 0.7102WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 60s 156ms/step - loss: 3.5632 - price_loss: 2.4553 - type_loss: 4.6712 - price_sparse_categorical_accuracy: 0.7051 - type_sparse_categorical_accuracy: 0.7102 - val_loss: 3.5358 - val_price_loss: 2.8492 - val_type_loss: 4.2224 - val_price_sparse_categorical_accuracy: 0.5845 - val_type_sparse_categorical_accuracy: 0.7110\n",
            "Epoch 17/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 2.9052 - price_loss: 1.8884 - type_loss: 3.9220 - price_sparse_categorical_accuracy: 0.7241 - type_sparse_categorical_accuracy: 0.7151WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 60s 156ms/step - loss: 2.9052 - price_loss: 1.8884 - type_loss: 3.9220 - price_sparse_categorical_accuracy: 0.7241 - type_sparse_categorical_accuracy: 0.7151 - val_loss: 6.4211 - val_price_loss: 2.7976 - val_type_loss: 10.0446 - val_price_sparse_categorical_accuracy: 0.6448 - val_type_sparse_categorical_accuracy: 0.4174\n",
            "Epoch 18/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 10.3929 - price_loss: 4.6745 - type_loss: 16.1114 - price_sparse_categorical_accuracy: 0.6776 - type_sparse_categorical_accuracy: 0.6597WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 60s 156ms/step - loss: 10.3929 - price_loss: 4.6745 - type_loss: 16.1114 - price_sparse_categorical_accuracy: 0.6776 - type_sparse_categorical_accuracy: 0.6597 - val_loss: 79.4091 - val_price_loss: 44.7735 - val_type_loss: 114.0448 - val_price_sparse_categorical_accuracy: 0.6232 - val_type_sparse_categorical_accuracy: 0.7143\n",
            "Epoch 19/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 18.1719 - price_loss: 8.6533 - type_loss: 27.6905 - price_sparse_categorical_accuracy: 0.6460 - type_sparse_categorical_accuracy: 0.6604WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 60s 156ms/step - loss: 18.1719 - price_loss: 8.6533 - type_loss: 27.6905 - price_sparse_categorical_accuracy: 0.6460 - type_sparse_categorical_accuracy: 0.6604 - val_loss: 6.5354 - val_price_loss: 4.2047 - val_type_loss: 8.8661 - val_price_sparse_categorical_accuracy: 0.4928 - val_type_sparse_categorical_accuracy: 0.7241\n",
            "Epoch 20/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 5.8115 - price_loss: 3.0022 - type_loss: 8.6207 - price_sparse_categorical_accuracy: 0.7125 - type_sparse_categorical_accuracy: 0.7194WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n",
            "382/382 [==============================] - 60s 157ms/step - loss: 5.8115 - price_loss: 3.0022 - type_loss: 8.6207 - price_sparse_categorical_accuracy: 0.7125 - type_sparse_categorical_accuracy: 0.7194 - val_loss: 5.8970 - val_price_loss: 3.3705 - val_type_loss: 8.4235 - val_price_sparse_categorical_accuracy: 0.6488 - val_type_sparse_categorical_accuracy: 0.7038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Impact:***\n",
        "\n",
        "Changing the dictionary size and the vector space for each word in the embeddings layer made an increase in accuracy to 54%"
      ],
      "metadata": {
        "id": "8-hTcyr5Xmmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Trial 3:***\n",
        "For the third trial, we used a different architecture for processing our image data by using 2 convolution layers, each followed by a max pooling layer, then adding a dense layer and a dropout layer of 0.2 "
      ],
      "metadata": {
        "id": "SZ0DmTZNN2It"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Dropout\n",
        "##Tokenizer\n",
        "vocab_size = 14000\n",
        "max_len = 128\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "x_train_text_id = _preprocess(x_train_text)\n",
        "\n",
        "##Model Building\n",
        "in_text = keras.Input(batch_shape=(None, max_len)) \n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 200)(in_text) \n",
        "averaged = tf.reduce_mean(embedded, axis=1)\n",
        "\n",
        "\n",
        "conv1=Conv2D(32, (3,3), padding='same', activation=\"relu\")(in_image)\n",
        "pool1=MaxPool2D((2, 2), strides=2)(conv1)\n",
        "conv2=Conv2D(64, (3,3), padding='same', activation=\"relu\")(pool1)\n",
        "pool2=MaxPool2D((2, 2), strides=2)(conv2)\n",
        "flattened=Flatten()(pool2)\n",
        "dense=Dense(100, activation=\"relu\")(flattened)\n",
        "drop=Dropout(0.2)(dense)\n",
        "\n",
        "\n",
        "fused = tf.concat([averaged, drop], axis=-1)\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.5,\n",
        "        'price': 0.5,\n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "##Model Training\n",
        "history = model.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "##Model Testing and making predictions\n",
        "x_test_text = _preprocess(df_test.summary.astype('str'))\n",
        "y_predict = model.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "price_predicted = y_predict['price'] \n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "\n",
        "pd.DataFrame(\n",
        "    {'id': df_test.id,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdpIr155QUFf",
        "outputId": "12ad87fb-69bf-4bbc-8f6d-8f4a7f9ab760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "382/382 [==============================] - 60s 152ms/step - loss: 2.0164 - price_loss: 1.7064 - type_loss: 2.3263 - price_sparse_categorical_accuracy: 0.6047 - type_sparse_categorical_accuracy: 0.7468 - val_loss: 0.8618 - val_price_loss: 0.7806 - val_type_loss: 0.9429 - val_price_sparse_categorical_accuracy: 0.6363 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 2/20\n",
            "382/382 [==============================] - 58s 151ms/step - loss: 0.8515 - price_loss: 0.7541 - type_loss: 0.9488 - price_sparse_categorical_accuracy: 0.6586 - type_sparse_categorical_accuracy: 0.7538 - val_loss: 0.8034 - val_price_loss: 0.7275 - val_type_loss: 0.8792 - val_price_sparse_categorical_accuracy: 0.6815 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 3/20\n",
            "382/382 [==============================] - 58s 151ms/step - loss: 0.7677 - price_loss: 0.6777 - type_loss: 0.8576 - price_sparse_categorical_accuracy: 0.7014 - type_sparse_categorical_accuracy: 0.7569 - val_loss: 0.7737 - val_price_loss: 0.7156 - val_type_loss: 0.8318 - val_price_sparse_categorical_accuracy: 0.6913 - val_type_sparse_categorical_accuracy: 0.7733\n",
            "Epoch 4/20\n",
            "382/382 [==============================] - 58s 152ms/step - loss: 0.6973 - price_loss: 0.6177 - type_loss: 0.7770 - price_sparse_categorical_accuracy: 0.7400 - type_sparse_categorical_accuracy: 0.7732 - val_loss: 0.7455 - val_price_loss: 0.6917 - val_type_loss: 0.7992 - val_price_sparse_categorical_accuracy: 0.6946 - val_type_sparse_categorical_accuracy: 0.7779\n",
            "Epoch 5/20\n",
            "382/382 [==============================] - 58s 151ms/step - loss: 0.6338 - price_loss: 0.5659 - type_loss: 0.7016 - price_sparse_categorical_accuracy: 0.7651 - type_sparse_categorical_accuracy: 0.7945 - val_loss: 0.7334 - val_price_loss: 0.6943 - val_type_loss: 0.7725 - val_price_sparse_categorical_accuracy: 0.6913 - val_type_sparse_categorical_accuracy: 0.7831\n",
            "Epoch 6/20\n",
            "382/382 [==============================] - 58s 152ms/step - loss: 0.5737 - price_loss: 0.5178 - type_loss: 0.6295 - price_sparse_categorical_accuracy: 0.7876 - type_sparse_categorical_accuracy: 0.8176 - val_loss: 0.7266 - val_price_loss: 0.6998 - val_type_loss: 0.7534 - val_price_sparse_categorical_accuracy: 0.6940 - val_type_sparse_categorical_accuracy: 0.7955\n",
            "Epoch 7/20\n",
            "382/382 [==============================] - 58s 151ms/step - loss: 0.5232 - price_loss: 0.4778 - type_loss: 0.5685 - price_sparse_categorical_accuracy: 0.8128 - type_sparse_categorical_accuracy: 0.8354 - val_loss: 0.7269 - val_price_loss: 0.7131 - val_type_loss: 0.7407 - val_price_sparse_categorical_accuracy: 0.6933 - val_type_sparse_categorical_accuracy: 0.7975\n",
            "Epoch 8/20\n",
            "382/382 [==============================] - 58s 151ms/step - loss: 0.4753 - price_loss: 0.4368 - type_loss: 0.5138 - price_sparse_categorical_accuracy: 0.8284 - type_sparse_categorical_accuracy: 0.8553 - val_loss: 0.7411 - val_price_loss: 0.7434 - val_type_loss: 0.7387 - val_price_sparse_categorical_accuracy: 0.6743 - val_type_sparse_categorical_accuracy: 0.7982\n",
            "Epoch 9/20\n",
            "382/382 [==============================] - 58s 151ms/step - loss: 0.4329 - price_loss: 0.3999 - type_loss: 0.4658 - price_sparse_categorical_accuracy: 0.8440 - type_sparse_categorical_accuracy: 0.8674 - val_loss: 0.7408 - val_price_loss: 0.7457 - val_type_loss: 0.7360 - val_price_sparse_categorical_accuracy: 0.6868 - val_type_sparse_categorical_accuracy: 0.7995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Impact:***\n",
        "\n",
        "Changing the architecture of the CNN and adding the dropout layer increased the accuracy to 56%"
      ],
      "metadata": {
        "id": "cWTusDjhrOi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Trial 4: ***\n",
        "\n",
        "For the forth layer, we used GRU for our text data by adding 2 GRU layers, each of 75 units"
      ],
      "metadata": {
        "id": "8SVYaXQkYVDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Tokenizer\n",
        "vocab_size = 14000\n",
        "max_len = 128\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "x_train_text_id = _preprocess(x_train_text)\n",
        "\n",
        "##Model Building\n",
        "in_text = keras.Input(batch_shape=(None, max_len)) \n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 200)(in_text) \n",
        "output = GRU(75, return_sequences=True)(embedded)\n",
        "output2 = GRU(75, return_sequences=True)(output)\n",
        "averaged = tf.reduce_mean(output2, axis=1)\n",
        "\n",
        "conv1=Conv2D(32, (3,3), padding='same', activation=\"relu\")(in_image)\n",
        "pool1=MaxPool2D((2, 2), strides=2)(conv1)\n",
        "conv2=Conv2D(64, (3,3), padding='same', activation=\"relu\")(pool1)\n",
        "pool2=MaxPool2D((2, 2), strides=2)(conv2)\n",
        "flattened=Flatten()(pool2)\n",
        "dense=Dense(100, activation=\"relu\")(flattened)\n",
        "drop=Dropout(0.2)(dense)\n",
        "\n",
        "\n",
        "fused = tf.concat([averaged, drop], axis=-1)\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.5,\n",
        "        'price': 0.5,\n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "##Model Training\n",
        "history = model.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "##Model Testing and making predictions\n",
        "x_test_text = _preprocess(df_test.summary.astype('str'))\n",
        "y_predict = model.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "price_predicted = y_predict['price'] \n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "\n",
        "pd.DataFrame(\n",
        "    {'id': df_test.id,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "88QGZ_3HvrKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Impact:***\n",
        "\n",
        "Using GRU made the accuracy drop to 51%"
      ],
      "metadata": {
        "id": "YhuXxSxzLlVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Trial 5:***\n",
        "\n",
        "For the fifth trial, we decided to use the bidirectional model with LSTM. We added two layers of Bidirectional LSTM, each of 16 units"
      ],
      "metadata": {
        "id": "0ZhuPzAFZ9Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Tokenizer\n",
        "vocab_size = 14000\n",
        "max_len = 128\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "x_train_text_id = _preprocess(x_train_text)\n",
        "\n",
        "##Model Building\n",
        "in_text = keras.Input(batch_shape=(None, max_len)) \n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 200)(in_text) \n",
        "x = Bidirectional(LSTM(16, return_sequences=True))(embedded)\n",
        "x = Bidirectional(LSTM(16))(x)\n",
        "#averaged = tf.reduce_mean(x, axis=1)\n",
        "\n",
        "conv1=Conv2D(32, (3,3), padding='same', activation=\"relu\")(in_image)\n",
        "pool1=MaxPool2D((2, 2), strides=2)(conv1)\n",
        "conv2=Conv2D(64, (3,3), padding='same', activation=\"relu\")(pool1)\n",
        "pool2=MaxPool2D((2, 2), strides=2)(conv2)\n",
        "flattened=Flatten()(pool2)\n",
        "dense=Dense(100, activation=\"relu\")(flattened)\n",
        "drop=Dropout(0.2)(dense)\n",
        "\n",
        "\n",
        "fused = tf.concat([x, drop], axis=-1)\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.5,\n",
        "        'price': 0.5,\n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "##Model Training\n",
        "history = model.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "##Model Testing and making predictions\n",
        "x_test_text = _preprocess(df_test.summary.astype('str'))\n",
        "y_predict = model.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "price_predicted = y_predict['price'] \n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "\n",
        "pd.DataFrame(\n",
        "    {'id': df_test.id,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwK3pQi5BEt6",
        "outputId": "5261abe2-4875-4c7b-c92a-c1b5f8c6fa83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "382/382 [==============================] - 114s 278ms/step - loss: 1.8490 - price_loss: 1.9608 - type_loss: 1.7373 - price_sparse_categorical_accuracy: 0.6153 - type_sparse_categorical_accuracy: 0.7443 - val_loss: 0.8363 - val_price_loss: 0.7880 - val_type_loss: 0.8846 - val_price_sparse_categorical_accuracy: 0.6514 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 2/20\n",
            "382/382 [==============================] - 105s 274ms/step - loss: 0.7644 - price_loss: 0.6980 - type_loss: 0.8308 - price_sparse_categorical_accuracy: 0.6950 - type_sparse_categorical_accuracy: 0.7607 - val_loss: 0.7826 - val_price_loss: 0.7210 - val_type_loss: 0.8442 - val_price_sparse_categorical_accuracy: 0.6809 - val_type_sparse_categorical_accuracy: 0.7713\n",
            "Epoch 3/20\n",
            "382/382 [==============================] - 103s 270ms/step - loss: 0.6554 - price_loss: 0.5947 - type_loss: 0.7162 - price_sparse_categorical_accuracy: 0.7605 - type_sparse_categorical_accuracy: 0.7966 - val_loss: 0.7978 - val_price_loss: 0.7386 - val_type_loss: 0.8569 - val_price_sparse_categorical_accuracy: 0.6723 - val_type_sparse_categorical_accuracy: 0.7654\n",
            "Epoch 4/20\n",
            "382/382 [==============================] - 103s 270ms/step - loss: 0.5660 - price_loss: 0.5096 - type_loss: 0.6224 - price_sparse_categorical_accuracy: 0.8058 - type_sparse_categorical_accuracy: 0.8267 - val_loss: 0.8373 - val_price_loss: 0.7860 - val_type_loss: 0.8887 - val_price_sparse_categorical_accuracy: 0.6815 - val_type_sparse_categorical_accuracy: 0.7543\n",
            "Epoch 5/20\n",
            "382/382 [==============================] - 102s 268ms/step - loss: 0.4963 - price_loss: 0.4450 - type_loss: 0.5475 - price_sparse_categorical_accuracy: 0.8322 - type_sparse_categorical_accuracy: 0.8526 - val_loss: 0.8880 - val_price_loss: 0.8361 - val_type_loss: 0.9400 - val_price_sparse_categorical_accuracy: 0.6782 - val_type_sparse_categorical_accuracy: 0.7654\n",
            "Epoch 6/20\n",
            "382/382 [==============================] - 109s 286ms/step - loss: 0.4454 - price_loss: 0.4085 - type_loss: 0.4823 - price_sparse_categorical_accuracy: 0.8451 - type_sparse_categorical_accuracy: 0.8723 - val_loss: 0.9436 - val_price_loss: 0.9107 - val_type_loss: 0.9764 - val_price_sparse_categorical_accuracy: 0.6586 - val_type_sparse_categorical_accuracy: 0.7503\n",
            "Epoch 7/20\n",
            "382/382 [==============================] - 103s 271ms/step - loss: 0.3992 - price_loss: 0.3564 - type_loss: 0.4419 - price_sparse_categorical_accuracy: 0.8692 - type_sparse_categorical_accuracy: 0.8826 - val_loss: 0.9718 - val_price_loss: 0.9544 - val_type_loss: 0.9891 - val_price_sparse_categorical_accuracy: 0.6579 - val_type_sparse_categorical_accuracy: 0.7523\n",
            "Epoch 8/20\n",
            "382/382 [==============================] - 104s 272ms/step - loss: 0.3589 - price_loss: 0.3187 - type_loss: 0.3991 - price_sparse_categorical_accuracy: 0.8774 - type_sparse_categorical_accuracy: 0.8979 - val_loss: 1.0261 - val_price_loss: 1.0586 - val_type_loss: 0.9935 - val_price_sparse_categorical_accuracy: 0.6265 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 9/20\n",
            "382/382 [==============================] - 106s 278ms/step - loss: 0.3260 - price_loss: 0.2939 - type_loss: 0.3581 - price_sparse_categorical_accuracy: 0.8858 - type_sparse_categorical_accuracy: 0.9090 - val_loss: 1.0485 - val_price_loss: 1.0716 - val_type_loss: 1.0254 - val_price_sparse_categorical_accuracy: 0.6540 - val_type_sparse_categorical_accuracy: 0.7654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Impact:***\n",
        "\n",
        "Using LSTM achieved an accuracy of 53%, which is better than that of GRU but still lower that achieved using embedding on their own with no GRU or bidirectional layers."
      ],
      "metadata": {
        "id": "s3WKNYaBwWnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Trial 6:***\n",
        "\n",
        "For the sixth trial, we tried using CNN with text data. We added a 1D convolution layer of 250 units, followed by a max pooling layer, a dense and a dropout layer. "
      ],
      "metadata": {
        "id": "WTRfVgS4afNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Tokenizer\n",
        "vocab_size = 14000\n",
        "max_len = 128\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "x_train_text_id = _preprocess(x_train_text)\n",
        "\n",
        "##Model Building\n",
        "in_text = keras.Input(batch_shape=(None, max_len)) \n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 200)(in_text) \n",
        "out1=Conv1D(250, 3, activation='relu')(embedded)\n",
        "out2=GlobalMaxPooling1D()(out1)\n",
        "out3=Dense(250, activation='relu')(out2)\n",
        "drop1=Dropout(0.2)(out3)\n",
        "#averaged = tf.reduce_mean(out3, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "conv1=Conv2D(32, (3,3), padding='same', activation=\"relu\")(in_image)\n",
        "pool1=MaxPool2D((2, 2), strides=2)(conv1)\n",
        "conv2=Conv2D(64, (3,3), padding='same', activation=\"relu\")(pool1)\n",
        "pool2=MaxPool2D((2, 2), strides=2)(conv2)\n",
        "flattened=Flatten()(pool2)\n",
        "dense=Dense(100, activation=\"relu\")(flattened)\n",
        "drop=Dropout(0.2)(dense)\n",
        "\n",
        "\n",
        "fused = tf.concat([drop1, drop], axis=-1)\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.2,\n",
        "        'price': 0.8,\n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "##Model Training\n",
        "history = model.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "##Model Testing and making predictions\n",
        "x_test_text = _preprocess(df_test.summary.astype('str'))\n",
        "x_test_image = np.array([load_image(i) for i in tqdm(df_test.image)])\n",
        "\n",
        "y_predict = model.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "price_predicted = y_predict['price'] \n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "\n",
        "pd.DataFrame(\n",
        "    {'id': df_test.id,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312,
          "referenced_widgets": [
            "d47010a71d53401fb62152b3845cac3f",
            "5cee952f68634d32a56cbad8c1e5e98f",
            "0ca94012a6354623ab902eeca4b02036",
            "704f3edc74d14a20909c82bcb9e34172",
            "023d9c5ff45d4522a883939b29d84608",
            "94a94ee09c0343418247de17253355e6",
            "5028f22cd9074562a5f92da98d5cea12",
            "fdff1c1fd8db40c597305f9cd3c62ca1",
            "623897cab1714a97b990e8c40786d790",
            "75701240738c4182bbbe12cd738550c7",
            "a0a6bd128c5849279ba5c173526097c1"
          ]
        },
        "id": "G3SNhqoQ0Dun",
        "outputId": "7dae4e9d-19c5-446e-be05-d6bc3ceb7f34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "382/382 [==============================] - 75s 193ms/step - loss: 3.7810 - price_loss: 4.0660 - type_loss: 2.6411 - price_sparse_categorical_accuracy: 0.6265 - type_sparse_categorical_accuracy: 0.7445 - val_loss: 0.7537 - val_price_loss: 0.7124 - val_type_loss: 0.9187 - val_price_sparse_categorical_accuracy: 0.6953 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 2/20\n",
            "382/382 [==============================] - 72s 188ms/step - loss: 0.6833 - price_loss: 0.6246 - type_loss: 0.9181 - price_sparse_categorical_accuracy: 0.7369 - type_sparse_categorical_accuracy: 0.7551 - val_loss: 0.7241 - val_price_loss: 0.6997 - val_type_loss: 0.8216 - val_price_sparse_categorical_accuracy: 0.6953 - val_type_sparse_categorical_accuracy: 0.7720\n",
            "Epoch 3/20\n",
            "382/382 [==============================] - 72s 188ms/step - loss: 0.4812 - price_loss: 0.4000 - type_loss: 0.8061 - price_sparse_categorical_accuracy: 0.8522 - type_sparse_categorical_accuracy: 0.7738 - val_loss: 0.8110 - val_price_loss: 0.8153 - val_type_loss: 0.7939 - val_price_sparse_categorical_accuracy: 0.6750 - val_type_sparse_categorical_accuracy: 0.7798\n",
            "Epoch 4/20\n",
            "382/382 [==============================] - 72s 189ms/step - loss: 0.3168 - price_loss: 0.2256 - type_loss: 0.6815 - price_sparse_categorical_accuracy: 0.9267 - type_sparse_categorical_accuracy: 0.8004 - val_loss: 0.9563 - val_price_loss: 0.9949 - val_type_loss: 0.8021 - val_price_sparse_categorical_accuracy: 0.6376 - val_type_sparse_categorical_accuracy: 0.7857\n",
            "Epoch 5/20\n",
            "382/382 [==============================] - 72s 189ms/step - loss: 0.2335 - price_loss: 0.1592 - type_loss: 0.5308 - price_sparse_categorical_accuracy: 0.9513 - type_sparse_categorical_accuracy: 0.8397 - val_loss: 0.9752 - val_price_loss: 1.0241 - val_type_loss: 0.7797 - val_price_sparse_categorical_accuracy: 0.6887 - val_type_sparse_categorical_accuracy: 0.7995\n",
            "Epoch 6/20\n",
            "382/382 [==============================] - 72s 188ms/step - loss: 0.1888 - price_loss: 0.1336 - type_loss: 0.4098 - price_sparse_categorical_accuracy: 0.9572 - type_sparse_categorical_accuracy: 0.8805 - val_loss: 0.9935 - val_price_loss: 1.0353 - val_type_loss: 0.8261 - val_price_sparse_categorical_accuracy: 0.6881 - val_type_sparse_categorical_accuracy: 0.7916\n",
            "Epoch 7/20\n",
            "382/382 [==============================] - 72s 190ms/step - loss: 0.1459 - price_loss: 0.1049 - type_loss: 0.3098 - price_sparse_categorical_accuracy: 0.9638 - type_sparse_categorical_accuracy: 0.9153 - val_loss: 1.1297 - val_price_loss: 1.1859 - val_type_loss: 0.9048 - val_price_sparse_categorical_accuracy: 0.6966 - val_type_sparse_categorical_accuracy: 0.7929\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7360 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d47010a71d53401fb62152b3845cac3f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Impact:***\n",
        "\n",
        "Using the CNN with the text feature achieved an accuracy of 49%, which is not an improvement compared to previously used models."
      ],
      "metadata": {
        "id": "kpmqCcnidVyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Trial 7:***\n",
        "\n",
        "For the seventh trial, we used LSTM layers (not bidirectional) by adding 2 layers, each of 75 units. We also removed the averaging layer that was added after generating the embeddings. "
      ],
      "metadata": {
        "id": "0C8zvVbAbKVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Tokenizer\n",
        "vocab_size = 14000\n",
        "max_len = 128\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "x_train_text_id = _preprocess(x_train_text)\n",
        "\n",
        "##Model Building\n",
        "in_text = keras.Input(batch_shape=(None, max_len)) \n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 200)(in_text) \n",
        "output = LSTM(75, return_sequences=True)(embedded)\n",
        "output2 = LSTM(75)(output)\n",
        "\n",
        "conv1=Conv2D(32, (3,3), padding='same', activation=\"relu\")(in_image)\n",
        "pool1=MaxPool2D((2, 2), strides=2)(conv1)\n",
        "conv2=Conv2D(64, (3,3), padding='same', activation=\"relu\")(pool1)\n",
        "pool2=MaxPool2D((2, 2), strides=2)(conv2)\n",
        "flattened=Flatten()(pool2)\n",
        "dense=Dense(100, activation=\"relu\")(flattened)\n",
        "drop=Dropout(0.2)(dense)\n",
        "\n",
        "\n",
        "fused = tf.concat([output2, drop], axis=-1)\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.5,\n",
        "        'price': 0.5,\n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "##Model Training\n",
        "history = model.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "##Model Testing and making predictions\n",
        "x_test_image = np.array([load_image(i) for i in tqdm(df_test.image)])\n",
        "x_test_text = _preprocess(df_test.summary.astype('str'))\n",
        "y_predict = model.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "price_predicted = y_predict['price'] \n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "\n",
        "pd.DataFrame(\n",
        "    {'id': df_test.id,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K53_u9RJeCCg",
        "outputId": "de42123f-27fd-41c5-efe5-c53925212006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "382/382 [==============================] - 130s 328ms/step - loss: 2.8148 - price_loss: 3.0009 - type_loss: 2.6287 - price_sparse_categorical_accuracy: 0.6073 - type_sparse_categorical_accuracy: 0.7377 - val_loss: 0.9093 - val_price_loss: 0.8403 - val_type_loss: 0.9783 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 2/20\n",
            "382/382 [==============================] - 124s 323ms/step - loss: 0.9299 - price_loss: 0.8379 - type_loss: 1.0219 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9026 - val_price_loss: 0.8315 - val_type_loss: 0.9736 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 3/20\n",
            "382/382 [==============================] - 126s 331ms/step - loss: 0.9274 - price_loss: 0.8339 - type_loss: 1.0208 - price_sparse_categorical_accuracy: 0.6171 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9060 - val_price_loss: 0.8252 - val_type_loss: 0.9869 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 4/20\n",
            "382/382 [==============================] - 124s 326ms/step - loss: 0.9258 - price_loss: 0.8344 - type_loss: 1.0171 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9069 - val_price_loss: 0.8253 - val_type_loss: 0.9886 - val_price_sparse_categorical_accuracy: 0.6245 - val_type_sparse_categorical_accuracy: 0.7641\n",
            "Epoch 5/20\n",
            "382/382 [==============================] - 124s 324ms/step - loss: 0.9238 - price_loss: 0.8306 - type_loss: 1.0171 - price_sparse_categorical_accuracy: 0.6199 - type_sparse_categorical_accuracy: 0.7538 - val_loss: 0.9073 - val_price_loss: 0.8297 - val_type_loss: 0.9849 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 6/20\n",
            "382/382 [==============================] - 125s 327ms/step - loss: 0.9260 - price_loss: 0.8331 - type_loss: 1.0190 - price_sparse_categorical_accuracy: 0.6202 - type_sparse_categorical_accuracy: 0.7538 - val_loss: 0.9093 - val_price_loss: 0.8343 - val_type_loss: 0.9843 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 7/20\n",
            "382/382 [==============================] - 125s 326ms/step - loss: 0.9272 - price_loss: 0.8362 - type_loss: 1.0183 - price_sparse_categorical_accuracy: 0.6197 - type_sparse_categorical_accuracy: 0.7538 - val_loss: 0.9089 - val_price_loss: 0.8267 - val_type_loss: 0.9911 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 8/20\n",
            "382/382 [==============================] - 128s 334ms/step - loss: 0.9282 - price_loss: 0.8377 - type_loss: 1.0188 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9049 - val_price_loss: 0.8314 - val_type_loss: 0.9784 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Impact:***\n",
        "\n",
        "Using 2 LSTM layers made the accuracy go up to 62.010%, which is the best accuracy achieved so far by a big margin compared to previous models."
      ],
      "metadata": {
        "id": "6Vx4LO5wjC07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Trial 8:***\n",
        "\n",
        "For the eighth trial, we changed the model from a multimodal and multitasking model (using text and image data to predict the type and the price) to only using the text data (summary column) to predict the price. \n",
        "\n",
        "Moreover, we added a third LSTM layer of 75 units. The aim here is to assess whether the image data affects our prediction or not."
      ],
      "metadata": {
        "id": "4wkjqUaOb0hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Tokenizer\n",
        "vocab_size = 14000\n",
        "max_len = 128\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "x_train_text_id = _preprocess(x_train_text)\n",
        "\n",
        "##Model Building\n",
        "in_text = keras.Input(batch_shape=(None, max_len)) \n",
        "\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 200)(in_text) \n",
        "output =  LSTM(75, return_sequences=True)(embedded)\n",
        "output1 = LSTM(75, return_sequences=True)(output)\n",
        "output2 = LSTM(75)(output1)\n",
        "\n",
        "\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(output2)\n",
        "\n",
        "\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "    },\n",
        "    outputs={\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "\n",
        "    metrics={\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "##Model Training\n",
        "history = model.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "    },\n",
        "    y={\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "##Model Testing and making predictions\n",
        "x_test_text = _preprocess(df_test.summary.astype('str'))\n",
        "y_predict = model.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "    }\n",
        ")\n",
        "price_predicted = y_predict['price'] \n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "\n",
        "pd.DataFrame(\n",
        "    {'id': df_test.id,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "s5FRWLjMrb7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Impact:***\n",
        "\n",
        "Using 3 layers of LSTM alongside using text features only to predict the price made the accuracy reach 62.038% (which is better than the previous model but with a very small margin). This shows that the image data did not really affect the prediction process.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MPZtQ3zrAf7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Ideal Solution:***\n",
        "\n",
        "The model that achieved the best result was the LSTM model, with 3 layers each of 75 units while not using the image data in the prediction. Moreover, using a dictionary of 14000 words and a vector space of 200 dimensions for each word proved to be the optimal model."
      ],
      "metadata": {
        "id": "mArG3KbUjTIE"
      }
    }
  ]
}